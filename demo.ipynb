{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783029e5",
   "metadata": {},
   "source": [
    "# Future Object Detection\n",
    "\n",
    "This purpose of this notebook is to explore the capabilities of our Future Object Detection model. The model is trained on the [Nuscenes](https://www.nuscenes.org/) or [NuImages](https://www.nuscenes.org/) dataset, which sequences of images with bounding box annotations. The model takes 2 images at times (T-1, T), together with the corresponding ego-motion information, and predicts the 2D bounding boxes at time T+1.\n",
    "\n",
    "For full information, please see the paper [\"Future Object Detection with Spatiotemporal Transformers\"](https://arxiv.org/abs/2204.10321)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19509c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"./ConditionalDETR\")\n",
    "\n",
    "import cv2\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "from future_od.datasets import nu_scenes\n",
    "from future_od.models.st_detr import SpatioTemporalDETRArgs\n",
    "from future_od.utils.recursive_functions import recursive_to\n",
    "from future_od.utils.visualization import revert_imagenet_normalization, draw_boxes, COLOURS\n",
    "\n",
    "from runs._loader import get_nuim_loaders\n",
    "from runs._model import build_model\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f419f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe download weights\n",
    "CHECKPOINT_PATH = gdown.cached_download(\n",
    "    url=\"https://drive.google.com/file/d/1BkKvCfrJYORvRtPRAr5Uonltc4Nf4IGa\",\n",
    "    path=\"checkpoints/nuim_spatiotemporal_imu.pth.tar\",\n",
    "    quiet=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"  # \"cuda\"\n",
    "DEBUG = False  # Disable debug mode to run on the full dataset (recommended)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    device=DEVICE,\n",
    "    distributed=False, \n",
    "    debug=DEBUG,\n",
    "    night=False,\n",
    "    short_train=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "detr_args = SpatioTemporalDETRArgs(\n",
    "    pretrained_backbone=False,\n",
    "    num_classes=len(nu_scenes.CATEGORY_DICT),\n",
    "    num_queries=128,\n",
    "    lr_backbone=1e-4,\n",
    ")\n",
    "model = build_model(args, detr_args)\n",
    "checkpoint_dict = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint_dict[\"net\"])\n",
    "model.to(DEVICE)\n",
    "# Enable attention storage\n",
    "for layer in model._model.detector.decoder.layers:\n",
    "    for slot_to_image_attention in layer.image_attend:\n",
    "        slot_to_image_attention.store_attention = True\n",
    "pass\n",
    "train_loader, val_loaders = get_nuim_loaders(\n",
    "    (896, 1600), offsets=[-2, -1, 0], config=config, args=args, train_batch_size=1\n",
    ")\n",
    "val_loader = val_loaders['val0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d14e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample_idx, train=True):\n",
    "    data = train_loader.dataset[sample_idx] if train else val_loader.dataset[sample_idx]\n",
    "    for key in data:\n",
    "        if isinstance(data[key], torch.Tensor):\n",
    "            data[key] = data[key].unsqueeze(0)\n",
    "        else:\n",
    "            data[key] = [data[key]]\n",
    "    data = recursive_to(data, DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs, state, loss, stats, od_map_stuffs = model(data)\n",
    "    class_scores, boxes, video = outputs['class_scores'][0,0,0], outputs['boxes'][0,0,0], data['video'][0]\n",
    "    return outputs, class_scores, boxes, video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b21d02",
   "metadata": {},
   "source": [
    "## Visualize Future Object Detection(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a3d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, classes, boxes, labels=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        video (Tensor): Of size (3, H, W)\n",
    "        classes (Tensor or LongTensor): Of size (M, C) if Tensor or (M,) if LongTensor. 0 if background\n",
    "        boxes (Tensor): Of size (M, 4), encoded as (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    _, BACKGROUND_CLASS = classes.size()\n",
    "    vis = revert_imagenet_normalization(image)\n",
    "    if boxes is not None:\n",
    "        if isinstance(classes, (torch.FloatTensor, torch.cuda.FloatTensor)):  # We get logits\n",
    "            scores, classes = classes.max(dim=1)\n",
    "            classes[scores < 0.1] = BACKGROUND_CLASS\n",
    "        boxes = boxes[classes != BACKGROUND_CLASS]\n",
    "        colours = COLOURS[classes[classes != BACKGROUND_CLASS]]\n",
    "        vis = draw_boxes(vis, boxes, colours).permute(1, 2, 0).cpu().numpy().copy()\n",
    "        if labels is not None:\n",
    "            for label, box in zip(labels, boxes):\n",
    "                # Draw the idx on top of the box\n",
    "                cv2.putText(vis, str(label), (int(box[0]), int(box[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a577b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process random sample from the training set\n",
    "SAMPLE_IDX = np.random.randint(len(train_loader.dataset))\n",
    "CONFIDENCE_THRESHOLD = 0.3\n",
    "outputs, class_scores, boxes, video = process_sample(SAMPLE_IDX)\n",
    "\n",
    "# Threshold bounding boxes\n",
    "idxs = (class_scores[:,-1] > CONFIDENCE_THRESHOLD).nonzero().squeeze(1)\n",
    "score, box = class_scores[idxs], boxes[idxs]\n",
    "labels = idxs.cpu().numpy()\n",
    "\n",
    "# Plot Future Object Detections\n",
    "plt.figure(dpi=200, frameon=False)\n",
    "plt.axis('off')\n",
    "img = visualize(video[-1], score, box, labels)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527422e6",
   "metadata": {},
   "source": [
    "## Visualize attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(class_scores, boxes, video, obj_idx) -> None:\n",
    "    plt.figure(frameon=False,figsize=(16*3, 9), dpi=50)\n",
    "    # Plot past with attention\n",
    "    for frame_idx in range(video.shape[0]-1):\n",
    "        plt.subplot(131 + frame_idx)\n",
    "        att = model._model.detector.decoder.layers[-1].image_attend[1-frame_idx].stored_attention[0]\n",
    "        img = rearrange(revert_imagenet_normalization(video[frame_idx]), 'c h w -> h w c')\n",
    "        featuremap_shape = np.array((896, 1600))/32\n",
    "        att = rearrange(att, 'c (h w) -> c h w', h = int(featuremap_shape[0]), w = int(featuremap_shape[1]))\n",
    "        att_zoomed = torch.nn.functional.interpolate(att[None,:], scale_factor=32, mode='bilinear')[0].cpu().numpy()\n",
    "        plt.imshow(img.cpu().numpy(), interpolation='nearest')\n",
    "        plt.imshow(att_zoomed[obj_idx], alpha=np.clip(att_zoomed[obj_idx]*50, 0, 0.4), interpolation='nearest', cmap='coolwarm')\n",
    "        plt.axis('off')\n",
    "    # Plot future with bounding box\n",
    "    plt.subplot(133)\n",
    "    score = class_scores[obj_idx:obj_idx+1]\n",
    "    box = boxes[obj_idx:obj_idx+1]\n",
    "    img = visualize(video[-1], score, box)\n",
    "    plt.subplots_adjust(wspace=0.01, hspace=0)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_IDX = 6\n",
    "outputs, class_scores, boxes, video = process_sample(SAMPLE_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90222a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OBJ_RANK_IDX = 2\n",
    "OBJ_IDX_OVERRIDE = None\n",
    "\n",
    "obj_idxs = torch.topk(class_scores[:,-1], 10).indices.cpu().numpy()\n",
    "obj_idx = obj_idxs[OBJ_RANK_IDX]\n",
    "if OBJ_IDX_OVERRIDE is not None:\n",
    "    obj_idx = OBJ_IDX_OVERRIDE\n",
    "plot_attention(class_scores, boxes, video, obj_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fod')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac29da6c57a679ad634ed5cc78baa105d5c90ad5a8e7143b9c3c1d588accf79d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
